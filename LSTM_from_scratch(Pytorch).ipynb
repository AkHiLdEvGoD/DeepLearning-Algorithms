{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1kV7Qk03q0mY3Tfa4MLda",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkHiLdEvGoD/DeepLearning-Algorithms/blob/main/LSTM_from_scratch(Pytorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZElg1Op4X1WV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "timesteps = 10\n",
        "n_features = 7\n",
        "hidden_units = 16\n",
        "output_size = 1"
      ],
      "metadata": {
        "id": "KeuPgefmX7o5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn(batch_size,timesteps,n_features)\n",
        "y = torch.randint(0,2,(batch_size,),dtype=torch.float32)"
      ],
      "metadata": {
        "id": "TLT20OYrcHfe"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_init(shape):\n",
        "  return (torch.randn(shape)*0.1).detach().requires_grad_()"
      ],
      "metadata": {
        "id": "EaxLg5ydcbmC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forget Gate weights and biases\n",
        "Wf = weight_init((n_features,hidden_units))\n",
        "Uf = weight_init((hidden_units,hidden_units))\n",
        "bf = torch.zeros(1,hidden_units).detach().requires_grad_()\n",
        "\n",
        "# Input Gate weights and biases\n",
        "Wi = weight_init((n_features,hidden_units))\n",
        "Ui = weight_init((hidden_units,hidden_units))\n",
        "bi = torch.zeros(1,hidden_units).detach().requires_grad_()\n",
        "\n",
        "# Input cell state weights and biases\n",
        "Wc = weight_init((n_features,hidden_units))\n",
        "Uc = weight_init((hidden_units,hidden_units))\n",
        "bc = torch.zeros(1,hidden_units).detach().requires_grad_()\n",
        "\n",
        "# Output gate weights and biases\n",
        "Wo = weight_init((n_features,hidden_units))\n",
        "Uo = weight_init((hidden_units,hidden_units))\n",
        "bo = torch.zeros(1,hidden_units).detach().requires_grad_()\n",
        "\n",
        "# Output Linear Nueron weights and biases\n",
        "Wout = weight_init((hidden_units,output_size))\n",
        "bout = torch.zeros(1,output_size).detach().requires_grad_()"
      ],
      "metadata": {
        "id": "O7CAN6Jlccw9"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = [Wf,Uf,bf,Wi,Ui,bi,Wc,Uc,bc,Wo,Uo,bo,Wout,bout]"
      ],
      "metadata": {
        "id": "mKK2jkZYkVdH"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(X):\n",
        "  h_t = torch.zeros(batch_size,hidden_units)\n",
        "  c_t = torch.zeros(batch_size,hidden_units)\n",
        "\n",
        "  for t in range(timesteps):\n",
        "    X_t = X[:,t,:]\n",
        "    f_t = torch.sigmoid(X_t @ Wf + h_t @ Uf + bf)\n",
        "    i_t = torch.sigmoid(X_t @ Wi + h_t @ Ui + bi)\n",
        "    g_t = torch.tanh(X_t @ Wc + h_t @ Uc + bc)\n",
        "    o_t = torch.sigmoid(X_t @ Wo + h_t @ Uo + bo)\n",
        "\n",
        "    c_t = f_t * c_t + i_t * g_t\n",
        "    h_t = o_t * torch.tanh(c_t)\n",
        "\n",
        "  logits = torch.sigmoid(h_t @ Wout + bout)\n",
        "  return logits"
      ],
      "metadata": {
        "id": "YXHdrp5tfiEE"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30\n",
        "learning_rate = 0.05\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  logits = forward_pass(X)\n",
        "  loss = F.binary_cross_entropy(logits.squeeze(),y)\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for param in params:\n",
        "      param -= learning_rate*param.grad\n",
        "      param.grad.zero_()\n",
        "\n",
        "  preds = (logits > 0.5).float()\n",
        "  acc = (preds.squeeze() == y).float().mean()\n",
        "\n",
        "  print(f\"Epoch {epoch+1:02d} | Loss: {loss.item():.4f} | Acc: {acc.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0O4qSK4fkkp",
        "outputId": "7ec42318-6fbf-43ae-fcf9-55560284bf83"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Loss: 0.6887 | Acc: 0.4688\n",
            "Epoch 02 | Loss: 0.6885 | Acc: 0.4688\n",
            "Epoch 03 | Loss: 0.6883 | Acc: 0.5000\n",
            "Epoch 04 | Loss: 0.6882 | Acc: 0.5312\n",
            "Epoch 05 | Loss: 0.6880 | Acc: 0.5000\n",
            "Epoch 06 | Loss: 0.6879 | Acc: 0.5312\n",
            "Epoch 07 | Loss: 0.6877 | Acc: 0.5312\n",
            "Epoch 08 | Loss: 0.6876 | Acc: 0.5312\n",
            "Epoch 09 | Loss: 0.6874 | Acc: 0.5312\n",
            "Epoch 10 | Loss: 0.6873 | Acc: 0.5312\n",
            "Epoch 11 | Loss: 0.6871 | Acc: 0.5312\n",
            "Epoch 12 | Loss: 0.6870 | Acc: 0.5312\n",
            "Epoch 13 | Loss: 0.6868 | Acc: 0.5312\n",
            "Epoch 14 | Loss: 0.6867 | Acc: 0.5312\n",
            "Epoch 15 | Loss: 0.6865 | Acc: 0.5312\n",
            "Epoch 16 | Loss: 0.6864 | Acc: 0.5625\n",
            "Epoch 17 | Loss: 0.6863 | Acc: 0.5625\n",
            "Epoch 18 | Loss: 0.6861 | Acc: 0.5625\n",
            "Epoch 19 | Loss: 0.6860 | Acc: 0.5625\n",
            "Epoch 20 | Loss: 0.6859 | Acc: 0.5625\n",
            "Epoch 21 | Loss: 0.6858 | Acc: 0.5625\n",
            "Epoch 22 | Loss: 0.6856 | Acc: 0.5625\n",
            "Epoch 23 | Loss: 0.6855 | Acc: 0.5625\n",
            "Epoch 24 | Loss: 0.6854 | Acc: 0.5625\n",
            "Epoch 25 | Loss: 0.6853 | Acc: 0.5625\n",
            "Epoch 26 | Loss: 0.6851 | Acc: 0.5625\n",
            "Epoch 27 | Loss: 0.6850 | Acc: 0.5625\n",
            "Epoch 28 | Loss: 0.6849 | Acc: 0.5625\n",
            "Epoch 29 | Loss: 0.6848 | Acc: 0.5625\n",
            "Epoch 30 | Loss: 0.6847 | Acc: 0.5625\n"
          ]
        }
      ]
    }
  ]
}