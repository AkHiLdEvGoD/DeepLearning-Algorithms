{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkdSP/tHARlPluMDvAxZsq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkHiLdEvGoD/DeepLearning-Algorithms/blob/main/Seq2Seq_Encoder_Decoder(Pytorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1bswYFd2Qw_o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "vocab_size = 10\n",
        "input_timesteps = 5\n",
        "output_timesteps = 6\n",
        "embedding_dim = 8\n",
        "hidden_size = 16"
      ],
      "metadata": {
        "id": "N2YVBCWNQ2gC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Synthetic dataset of vocab size 10, input_seq = 5, output_seq = 6\n",
        "X = torch.randint(0,vocab_size,(batch_size,input_timesteps))\n",
        "Y = torch.randint(0,vocab_size,(batch_size,output_timesteps))"
      ],
      "metadata": {
        "id": "SPo8CXNCnmM1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_randn(shape):\n",
        "  return (torch.randn(shape)*0.1).detach().requires_grad_()"
      ],
      "metadata": {
        "id": "MoUZDVN3nKRP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding matrix\n",
        "E = gen_randn((vocab_size,embedding_dim))"
      ],
      "metadata": {
        "id": "OtVn0CQbnllv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder weights\n",
        "Wxh_enc = gen_randn((embedding_dim,hidden_size))\n",
        "Whh_enc = gen_randn((hidden_size,hidden_size))\n",
        "b_enc = gen_randn((1,hidden_size))\n",
        "\n",
        "# Decoder weights\n",
        "Wxh_dec = gen_randn((embedding_dim,hidden_size))\n",
        "Whh_dec = gen_randn((hidden_size,hidden_size))\n",
        "b_dec = gen_randn((1,hidden_size))\n",
        "\n",
        "# Softmax Layer Weights(Output Layer)\n",
        "W_out = gen_randn((hidden_size,vocab_size))\n",
        "b_out = gen_randn((1,vocab_size))"
      ],
      "metadata": {
        "id": "SkDWcITFn9gB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = [E,Wxh_enc,Whh_enc,b_enc,Wxh_dec,Whh_dec,b_dec,W_out,b_out]"
      ],
      "metadata": {
        "id": "vQt7Vpvsd9II"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq2seq_forward(X,Y):\n",
        "\n",
        "  # Encoder\n",
        "  h_enc = torch.zeros(batch_size,hidden_size)\n",
        "  for t in range(input_timesteps):\n",
        "    x_t = E[X[:,t]]\n",
        "    h_enc = torch.tanh(x_t @ Wxh_enc + h_enc @ Whh_enc + b_enc)\n",
        "\n",
        "  # Decoder\n",
        "  h_dec = h_enc\n",
        "  logits = []\n",
        "  for t in range(output_timesteps):\n",
        "    y_t = E[Y[:,t]]  # Teacher Forcing\n",
        "    h_dec = torch.tanh(y_t @ Wxh_dec + h_dec @ Whh_dec + b_dec)\n",
        "    logit = h_dec @ W_out + b_out\n",
        "    logits.append(logit)           # Didn't use softmax here as softmax is already applied inside cross entropy function of Pytorch\n",
        "\n",
        "  logits = torch.stack(logits,dim=1)\n",
        "  return logits"
      ],
      "metadata": {
        "id": "rOO6eUhXoqPV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "learning_rate = 0.1\n",
        "for epoch in range(epochs):\n",
        "  logits = seq2seq_forward(X,Y)\n",
        "  loss = F.cross_entropy(logits.view(-1,vocab_size),Y.view(-1)) # Flatten the logits and the target tensor for loss calculation\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for param in params:\n",
        "      param -= learning_rate*param.grad\n",
        "\n",
        "  print(f'Epoch : {epoch+1}, Loss : {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgH01_SEo_vV",
        "outputId": "539b4666-4c4d-4e24-be20-da91cb51f37f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1, Loss : 2.298929214477539\n",
            "Epoch : 2, Loss : 2.2979421615600586\n",
            "Epoch : 3, Loss : 2.2960033416748047\n",
            "Epoch : 4, Loss : 2.293179988861084\n",
            "Epoch : 5, Loss : 2.2895662784576416\n",
            "Epoch : 6, Loss : 2.2852776050567627\n",
            "Epoch : 7, Loss : 2.280440330505371\n",
            "Epoch : 8, Loss : 2.2751858234405518\n",
            "Epoch : 9, Loss : 2.2696382999420166\n",
            "Epoch : 10, Loss : 2.263906478881836\n",
            "Epoch : 11, Loss : 2.2580761909484863\n",
            "Epoch : 12, Loss : 2.252199411392212\n",
            "Epoch : 13, Loss : 2.2462828159332275\n",
            "Epoch : 14, Loss : 2.240281581878662\n",
            "Epoch : 15, Loss : 2.234083414077759\n",
            "Epoch : 16, Loss : 2.227499008178711\n",
            "Epoch : 17, Loss : 2.2202439308166504\n",
            "Epoch : 18, Loss : 2.2119228839874268\n",
            "Epoch : 19, Loss : 2.2020108699798584\n",
            "Epoch : 20, Loss : 2.189828634262085\n"
          ]
        }
      ]
    }
  ]
}