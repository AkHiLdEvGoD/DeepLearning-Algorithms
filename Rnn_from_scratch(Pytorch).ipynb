{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtoGUOEr+NkkNN1DPDzSa5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkHiLdEvGoD/DeepLearning-Algorithms/blob/main/Rnn_from_scratch(Pytorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "_BJ49GEFv7R_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_size = 5\n",
        "hidden_size = 10\n",
        "timesteps = 7\n",
        "batch_size = 32\n",
        "output_size = 1"
      ],
      "metadata": {
        "id": "DZIPDoOlOmPs"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.randn(batch_size,timesteps,feature_size)\n",
        "y = torch.randint(0,output_size+1,(batch_size,),dtype=torch.float32)"
      ],
      "metadata": {
        "id": "j1cPj0QOPGhj"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Wxh = (torch.randn(feature_size, hidden_size) * 0.1).detach().requires_grad_()\n",
        "Whh = (torch.randn(hidden_size, hidden_size) * 0.1).detach().requires_grad_()\n",
        "Why = (torch.randn(hidden_size, output_size) * 0.1).detach().requires_grad_()\n",
        "bh = torch.zeros(1, hidden_size).detach().requires_grad_()\n",
        "by = torch.zeros(1, output_size).detach().requires_grad_()"
      ],
      "metadata": {
        "id": "jXfgna4zPZz4"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(X):\n",
        "  h_t = torch.zeros(batch_size,hidden_size)\n",
        "  for t in range(timesteps):\n",
        "    x_t = X[:,t,:]\n",
        "    h_t = torch.tanh(x_t @ Wxh + h_t @ Whh + bh)\n",
        "  logits = torch.sigmoid(h_t @ Why + by)\n",
        "  return logits"
      ],
      "metadata": {
        "id": "-X6wC27KP4i8"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "learning_rate = 0.05\n",
        "for epoch in range(epochs):\n",
        "  logits = forward_pass(X)\n",
        "  loss = F.binary_cross_entropy(logits.squeeze(),y)\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for param in [Wxh,Whh,Why,bh,by]:\n",
        "      param -= learning_rate * param.grad\n",
        "      param.grad.zero_()\n",
        "  # print(logits[:10].detach().cpu().numpy())\n",
        "  # print(f'epoch : {epoch+1}, loss : {loss.item():.2f}')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    preds = (logits > 0.5).float()\n",
        "    acc = (preds.squeeze() == y).float().mean()\n",
        "    print(f'epoch: {epoch}, loss: {loss.item():.4f}, acc: {acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2PqL0OHSApt",
        "outputId": "3e85c793-0a2e-4422-8008-92418cfb3af0"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, loss: 0.6897, acc: 0.6250\n",
            "epoch: 1, loss: 0.6887, acc: 0.5938\n",
            "epoch: 2, loss: 0.6877, acc: 0.5312\n",
            "epoch: 3, loss: 0.6867, acc: 0.5312\n",
            "epoch: 4, loss: 0.6858, acc: 0.5312\n",
            "epoch: 5, loss: 0.6848, acc: 0.5312\n",
            "epoch: 6, loss: 0.6839, acc: 0.5625\n",
            "epoch: 7, loss: 0.6830, acc: 0.5625\n",
            "epoch: 8, loss: 0.6821, acc: 0.6250\n",
            "epoch: 9, loss: 0.6813, acc: 0.6250\n",
            "epoch: 10, loss: 0.6804, acc: 0.6250\n",
            "epoch: 11, loss: 0.6796, acc: 0.6250\n",
            "epoch: 12, loss: 0.6788, acc: 0.6250\n",
            "epoch: 13, loss: 0.6780, acc: 0.6250\n",
            "epoch: 14, loss: 0.6772, acc: 0.6250\n",
            "epoch: 15, loss: 0.6764, acc: 0.6250\n",
            "epoch: 16, loss: 0.6757, acc: 0.6250\n",
            "epoch: 17, loss: 0.6749, acc: 0.6562\n",
            "epoch: 18, loss: 0.6742, acc: 0.6562\n",
            "epoch: 19, loss: 0.6735, acc: 0.6562\n"
          ]
        }
      ]
    }
  ]
}