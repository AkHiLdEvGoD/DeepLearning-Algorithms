{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlXURF2FznY5mdTvmcayj/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkHiLdEvGoD/DeepLearning-Algorithms/blob/main/Bahdanau_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "AGHPrT5xmA6g"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch_size = B\n",
        "\n",
        "src_len = S (length of input sentence)\n",
        "\n",
        "trg_len = T (length of target sentence)\n",
        "\n",
        "emb_dim = E\n",
        "\n",
        "enc_hidden_dim = H\n",
        "\n",
        "dec_hidden_dim = D\n",
        "\n",
        "output_dim = V (vocab size of target)"
      ],
      "metadata": {
        "id": "1LcTmwt821gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):                                                       # Shapes\n",
        "  def __init__(self,input_dims,embd_dims,hidden_dims,dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(input_dims,embd_dims)\n",
        "    self.gru = nn.GRU(embd_dims,hidden_dims,bidirectional=True,batch_first=True)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,X):                                                          # [B,S]\n",
        "    embedded = self.dropout(self.embedding(X))                                  # [B,S,E]\n",
        "    outputs,hidden = self.gru(embedded)                                         # [B,S,2H], [2,B,H]\n",
        "    return outputs,hidden"
      ],
      "metadata": {
        "id": "rKr8RpCtnBlO"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAtttention(nn.Module):\n",
        "  def __init__(self,enc_hidden_dims,dec_hidden_dims):                           # [B,S,2H], [B,D]\n",
        "    super().__init__()\n",
        "    self.attention = nn.Linear(enc_hidden_dims*2 + dec_hidden_dims,dec_hidden_dims)\n",
        "    self.v = nn.Parameter(torch.rand(dec_hidden_dims))\n",
        "\n",
        "  def forward(self,hidden_dec,enc_outs):\n",
        "    seq_len = enc_outs.shape[1]\n",
        "    hidden_dec = hidden_dec.unsqueeze(1).repeat(1,seq_len,1)                    # [B,D] -> [B,1,D] -> [B,S,D]\n",
        "\n",
        "    energy = torch.tanh(self.attention(torch.cat(hidden_dec,enc_outs),dims=2))  # [B,S,D+2H] -> [B,S,attn_dims==D]\n",
        "\n",
        "    v = self.v.unsqueeze(0).unsqueeze(1)                                        # [1,1,D]\n",
        "    scores = torch.sum(v*energy,dim=2)                                          # [B,S]\n",
        "    attn_weights = torch.softmax(scores,dim=1)                                  # [B,S]\n",
        "\n",
        "    return attn_weights"
      ],
      "metadata": {
        "id": "aG7bX6CuspT4"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,output_dims,embd_dims,dropout,enc_hidden_dims,dec_hidden_dims,attention):\n",
        "    super().__init__()\n",
        "    self.output_dims = output_dims\n",
        "    self.attention = attention\n",
        "    self.embedding = nn.Embedding(output_dims,embd_dims)\n",
        "    self.gru = nn.GRU(2*enc_hidden_dims + embd_dims,dec_hidden_dims,batch_first=True)\n",
        "    self.fc = nn.Linear(2*enc_hidden_dims + embd_dims + dec_hidden_dims,output_dims)\n",
        "    self.dropouts = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,Y_t,hidden,enc_outs):\n",
        "    Y_t = Y_t.unsqueeze(1)                                                      # [B,1]\n",
        "    embedded = self.dropouts(self.embedding(Y_t))                               # [B,1,E]\n",
        "\n",
        "    alignment_weights = self.attention(enc_outs,hidden)                         # [B,S]\n",
        "    a = alignment_weights.unsqueeze(1)                                          # [B,1,S]\n",
        "\n",
        "    context = torch.bmm(a,enc_outs)                                             # [B,1,S] * [B,S,2H] = [B,1,2H]\n",
        "    gru_input = torch.cat((context,embedded),dims=2)                            # [B,1,E+2H]\n",
        "\n",
        "    outputs,hidden = self.gru(gru_input,hidden.unsqueeze(1))                    # [B,1,D], [1,B,D]\n",
        "    embedded = embedded.squeeze(1)                                              # [B,E]\n",
        "    output = outputs.squeeze(1)                                                 # [B,D]\n",
        "    context = context.squeeze(1)                                                # [B,2H]\n",
        "\n",
        "    pred = self.fc(torch.cat((output, context, embedded), dims=1))              # [B,V]\n",
        "    return pred, hidden.squeeze(0)                                              # [B,V], [B,D]"
      ],
      "metadata": {
        "id": "IkMmo46CMNlZ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self,encoder,decoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self,src,trg,teacher_forcing_ratio=0.5):                          # src : [B,S] , trg :[B,T]\n",
        "    batch_size = trg.shape[0]\n",
        "    seq_len = trg.shape[1]\n",
        "    trg_vocab_size = self.decoder.output_dims\n",
        "\n",
        "    outputs = torch.zeros(batch_size,seq_len,trg_vocab_size)                    # [B,T,V]\n",
        "\n",
        "    enc_outs,hidden = self.encoder(src)                                         # [B,S,2H] , [2,B,H]\n",
        "\n",
        "    hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)                 # [B,2H]\n",
        "    input = trg[:,0]                                                            # <sos>\n",
        "\n",
        "    for t in range(1,seq_len):                                                  # 1 -> T\n",
        "      output,hidden = self.decoder(input,hidden,enc_outs)                       # [B,V], [B,D]\n",
        "      outputs[:,t,:] = output\n",
        "\n",
        "      teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "      top1 = output.argmax(1)\n",
        "\n",
        "      input = trg[:,t] if teacher_force else top1\n",
        "\n",
        "    return outputs                                                              # [B,T,V]"
      ],
      "metadata": {
        "id": "9CzJ77R1neLW"
      },
      "execution_count": 64,
      "outputs": []
    }
  ]
}